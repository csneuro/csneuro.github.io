---
layout: article
title: Concept Learning
---

# Concept Learning
subtitle here

## Introduction

At present we cannot fully account for the ease with which humans apprehend new concepts. We are presented with such apparently sparse and incomplete data, yet we still form robust mental models that allow us to give rich structure to what we experience in the world. Our learning ability seems to violate a priori constraints on what can be inferred from only so much data. If we recognize that we have too little data to support an inference, the only way out is to realize that there is more information than we are accounting for. The question is how this is true.  We are interested in investigating this phenomenon for many reasons, among them being how it would inform our attempts to endow machines with human-like intelligence and the implications it could have in treating learning disorders and improving pedagogical practices. By viewing problems of induction and inductive learning as computational problems, we can use new methods of neurobiological data collection (i.e. fMRI) to probe learning phenomena at a basic level. The fast-mapping of words to objects by a toddler, for instance, can be interpreted as a computation performed on the sensory data received by the toddler.

A useful computation around which much study is centered is that of word learning - the mapping of words to concepts or objects - in children. Word learning models lend themselves easily to both experimentation and generalization to other conceptions of learning. Word learning models that have been applied in the past are often defined by their approach to the structure of pre-existing knowledge and the structure of inference, as well as the relationship between the two. A very simple model in the case of word learning is that of hypothesis elimination, in which positive pairings of words to objects support a plausible hypothesis space and negative pairings reduce this hypothesis space. Such a process is, in other words, deductive. Connective or associative theories of learning describe a complex structure of inference that grows with new data - often represented by a neural network with many hidden layers. Nativist theories, in contrast, place some prior structure and/or content in knowledge and describe computations performed with these pre-existing structures. As we will see, however, none of these models have yet encompassed all characteristics of word learning as observed in humans.

## Conclusion